---
created: 2024-09-01T07:59:01.090Z
updated: 2024-09-01T13:38:28.425Z
assigned: ""
progress: 0
tags:
  - 'AI Models'
  - 'local AI'
  - Microservices
due: 2024-09-02T00:00:00.000Z
started: 2024-09-01T00:00:00.000Z
---

# Microservices Optimization

- Aside from Ollama certain actions are under performing. 
- Pulling models with the chat with mlx repo is painfully slow
- Running OpenHands locally typically crashes after 1 or 2 turns
- Aider performs well when performing inference with Groq LPU

## Sub-tasks

- [ ] Identify Bottlenecks in current microservices
- [ ] Implement caching mechanisms for frequently used models
- [ ] Explore model quantization for faster loading times
- [ ] Explore model compression for reduced local storage cost

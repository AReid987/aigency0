services:
  tailscale-authkey:
    image: tailscale/tailscale:latest
    container_name: tailscale-authkey
    hostname: tailscale-docker
    environment:
      - TS_AUTHKEY=tskey-auth-kn4443UWS611CNTRL-TX74yvREWSAbjeZLKmFySAh7RggBFZ5XP
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
    volumes:
      - tailscale-authkey:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - net_admin
      - sys_module
    restart: unless-stopped
  nginx-authkey:
    image: nginx
    network_mode: service:tailscale-authkey
  dask-scheduler:
    image: twgsportsclub/dask-docker-arm64
    platform: linux/arm64
    ports:
      - "${DASK_SCHEDULER_PORT:-8786}:8786"
      - "${DASK_DASHBOARD_PORT:-8787}:8787"
    command: ["dask-scheduler"]
    networks:
      - aigency-net
  dask-worker:
    image: twgsportsclub/dask-docker-arm64
    platform: linux/arm64
    command: ["dask-worker", "tcp://dask-scheduler:8786"]
    depends_on:
      - dask-scheduler
    deploy:
      replicas: ${DASK_WORKERS:-1}
    networks:
      - aigency-net
  vllm:
    image: vllm-server
    build:
      context: ./packages/vllm-server
      dockerfile: Dockerfile.vllm
    ports:
      - "${VLLM_PORT:-8000}:8000"
    command:
      [
        "python",
        "-m",
        "vllm.entrypoints.api_server",
        "--model",
        "${VLLM_MODEL:-facebook/opt-125m}",
        "--host",
        "0.0.0.0",
      ]
    deploy:
      replicas: ${VLLM_INSTANCES:-1}
    networks:
      - aigency-net
volumes:
  tailscale-authkey:
    driver: local
networks:
  aigency-net:
    driver: bridge
#   ollama:
#     image: ollama/ollama:latest
#     ports:
#       - "11434:11434"
#     volumes:
#       - ollama-data:/root/.ollama
#       - ./check_ollama.sh:/check_ollama.sh
#     deploy:
#       resources:
#         limits:
#           memory: 16G  # Set an upper limit
#         reservations:
#           memory: 12G  # Guarantee this much memory
#     networks:
#       - ai-assistant-network
#     healthcheck:
#       test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 40s
#     environment:
#       - OLLAMA_HOST=0.0.0.0
#     # command: sh -c "ollama serve"

#   searxng:
#     image: searxng/searxng:latest
#     volumes:
#       - ./searxng:/etc/searxng:rw
#     ports:
#       - "4000:8080"
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   perplexica-backend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: backend.dockerfile
#       args:
#         - SEARXNG_API_URL=http://searxng:8080
#     depends_on:
#       - searxng
#       - ollama
#     ports:
#       - "3001:3001"
#     volumes:
#       - backend-dbstore:/home/perplexica/data
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   perplexica-frontend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: app.dockerfile
#       args:
#         - NEXT_PUBLIC_API_URL=http://127.0.0.1:3001/api
#         - NEXT_PUBLIC_WS_URL=ws://127.0.0.1:3001
#     depends_on:
#       - perplexica-backend
#     ports:
#       - "3000:3000"
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   aider:
#     build:
#       context: ./packages/aider
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       ollama:
#           condition: service_healthy
#     stdin_open: true
#     tty: true
#     ports:
#       - "8501:8501"
#     volumes:
#       - .:/app  # Mount the entire monorepo
#       - ./packages/aider/start_aider.sh:/start_aider.sh  # Mount the start script
#     deploy:
#       resources:
#         limits:
#           memory: 16G  # Set an upper limit
#         reservations:
#           memory: 12G  # Guarantee this much memory
#     working_dir: /app/packages/aider  # Set the working directory to the aider package
#     environment:
#       - OLLAMA_API_BASE=http://ollama:11434
#       # - AIDER_MODEL=deepseek-coder-v2:latest
#       - AIDER_MODEL=codellama:7b
#       - SKIP_AIDER_UPDATE=false
#     command: /start_aider.sh
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   agent-zero:
#     build:
#       context: ./packages/agent-zero/docker
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     stdin_open: true
#     tty: true
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"
#     ports:
#       - "5001:5001"
#       - "2222:22"
#     volumes:
#       - ./agent-zero/work_dir:/home/agentuser/work_dir
#     environment:
#       - OLLAMA_BASE_URL=http://ollama:11434
#     cap_add:
#       - SYS_ADMIN
#     # security_opt:
#     #   - seccomp:unconfined
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   open-hands:
#     image:
#       ghcr.io/all-hands-ai/openhands:main
#     environment:
#       - SANDBOX_USER_ID=${SANDBOX_USER_ID:-$(id -u)}  # Use the current user ID
#       - WORKSPACE_DIR="./packages/open-hands/"
#     volumes:
#       # - ${WORKSPACE_BASE:-$(pwd)/workspace}:/opt/workspace_base

#       - /var/run/docker.sock:/var/run/docker.sock
#     ports:
#       - "3333:3333"
#       - "3000:3333"
#     command: uvicorn openhands.server.listen:app --host 0.0.0.0 --port 3333
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

# networks:
#   ai-assistant-network:

# volumes:
#   backend-dbstore:
#   ollama-data:

# services:
#   ollama:
#     image: docker.io/ollama/ollama:latest
#     ports:
#       - 11434:11434
#   searxng:
#     image: docker.io/searxng/searxng:latest
#     volumes:
#       - ./searxng:/etc/searxng:rw
#     ports:
#       - 4000:8080
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   perplexica-backend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: backend.dockerfile
#       args:
#         - SEARXNG_API_URL=http://searxng:8080
#     depends_on:
#       - searxng
#     ports:
#       - 3001:3001
#     volumes:
#       - backend-dbstore:/home/perplexica/data
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   perplexica-frontend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: app.dockerfile
#       args:
#         - NEXT_PUBLIC_API_URL=http://127.0.0.1:3001/api
#         - NEXT_PUBLIC_WS_URL=ws://127.0.0.1:3001
#     depends_on:
#       - perplexica-backend
#     ports:
#       - 3000:3000
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   aider:
#     build:
#       context: ./packages/aider
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     ports:
#       - 5555:5555
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   agent-zero:
#     build:
#       context: ./packages/agent-zero/docker
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     ports:
#       - 5001:5001
#     networks:
#       - perplexica-network
#     restart: unless-stopped

# networks:
#   perplexica-network:

# volumes:
#   backend-dbstore:

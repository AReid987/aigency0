services:
  # caddy:
  #   image: caddy
  #   # ports:
  #   #   - "80:80"
  #   #   - "443:443"
  #   volumes:
  #     - ./Caddyfile:/etc/caddy/Caddyfile
  #     - caddy_data:/data
  #     - caddy_config:/config
  #   network_mode: service:tailscale
  #   container_name: caddy
    # depends_on:
    #   - unified-dashboard
  # unified-dashboard:
  #   build:
  #     context: ./packages/dashboard
  #     dockerfile: Dockerfile
  #   volumes:
  #     - ./dashboard:/app
  #   environment:
  #     - NODE_ENV=production
  #   network_mode: service:tailscale
  #   hostname: unified-dashboard
  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    volumes:
      - ./tailscale:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    environment:
      - TS_AUTHKEY=${TS_KEY}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_ROUTES=10.0.0.0/8
      - TS_HOSTNAME=aigency-tailnet
      - TS_EXTRA_ARGS=--hostname=aigency-tailnet --advertise-tags=tag:aigency
    network_mode: host
    restart: unless-stopped
  nginx:
    image: nginx
    network_mode: service:tailscale
    container_name: nginx
  dask-scheduler:
    image: twgsportsclub/dask-docker-arm64
    platform: linux/arm64
    command: ["dask", "scheduler"]
    network_mode: service:tailscale
    container_name: dask-scheduler
  dask-worker:
    image: twgsportsclub/dask-docker-arm64
    platform: linux/arm64
    command: ["dask", "worker", "localhost:8786"]
    deploy:
      replicas: ${DASK_WORKERS:-1}
    network_mode: service:tailscale
    container_name: dask-worker
    depends_on:
      - dask-scheduler
  ollama:
    image: ollama/ollama
    volumes:
      - ./ollama_models:/root/.ollama
    network_mode: service:tailscale
    container_name: ollama
  pull-model:
    image: genai-stack/pull-model:latest
    build:
      context: ./packages/ollama
      dockerfile: Dockerfile.pull_model
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://aigency-tailnet:11434}
      - LLM=${LLM:-codellama:7b}
    container_name: pull_model
    network_mode: service:tailscale
    tty: true
  aider:
    image: paulgauthier/aider-full
    pull_policy: always
    volumes:
      - .:/app
    environment:
      # - GROQ_API_KEY=${GROQ_API_KEY}
      - OLLAMA_API_BASE=http://aigency-tailnet:11434
      - MLX_API_BASE=http://host.docker.internal:5555
      - LLM_API_URL=http://host.docker.internal:5555/predict
      - ollama
    command: --cache-prompts --gui --model ollama/deepseek-coder-v2
    network_mode: service:tailscale
    container_name: aider
    depends_on:
      - ollama
  pytorch:
    build:
      context: ./packages/pytorch
      dockerfile: Dockerfile.pytorch
    volumes:
      - .:/app
    env_file:
      - .env
    container_name: pytorch
    command: python server.py
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    network_mode: service:tailscale
    container_name: redis
volumes:
  redis_data:
  ollama_models:
  caddy_data:
  caddy_config:



  # coder:
  #   # This MUST be stable for our documentation and
  #   # other automations.
  #   image: ghcr.io/coder/coder:latest
  #   ports:
  #     - "7070:7070"
  #   environment:
  #     CODER_PG_CONNECTION_URL: "postgresql://${POSTGRES_USER:-username}:${POSTGRES_PASSWORD:-password}@database/${POSTGRES_DB:-coder}?sslmode=disable"
  #     CODER_HTTP_ADDRESS: "0.0.0.0:7777"
  #     # You'll need to set CODER_ACCESS_URL to an IP or domain
  #     # that workspaces can reach. This cannot be localhost
  #     # or 127.0.0.1 for non-Docker templates!
  #     CODER_ACCESS_URL: "${CODER_ACCESS_URL}"
  #   # If the coder user does not have write permissions on
  #   # the docker socket, you can uncomment the following
  #   # lines and set the group ID to one that has write
  #   # permissions on the docker socket.
  #   group_add:
  #     - ${DOCKER_GROUP:-998} # docker group on host
  #   volumes:
  #     - ${HOME}/.config/coderV2-docker:/home/coder/.config
  #     - /var/run/docker.sock:/var/run/docker.sock
  #   depends_on:
  #     database:
  #       condition: service_healthy
# database:
#     # Minimum supported version is 13.
#     # More versions here: https://hub.docker.com/_/postgres
#     image: "postgres:16"
#     ports:
#       - "5432:5432"
#     environment:
#       POSTGRES_USER: ${POSTGRES_USER:-username} # The PostgreSQL user (useful to connect to the database)
#       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password} # The PostgreSQL password (useful to connect to the database)
#       POSTGRES_DB: ${POSTGRES_DB:-coder} # The PostgreSQL default database (automatically created at first launch)
#     volumes:
#       - coder_data:/var/lib/postgresql/data # Use "docker volume rm coder_coder_data" to reset Coder
#     healthcheck:
#       test:
#         [
#           "CMD-SHELL",
#           "pg_isready -U ${POSTGRES_USER:-username} -d ${POSTGRES_DB:-coder}",
#         ]
#       interval: 5s
#       timeout: 5s
#       retries: 5
#   tailscale:
#     driver: local
# networks:
#   aigency-net:
#     driver: bridge
#       - aigency-net


# mlx:
#     build:
#       context: .
#       dockerfile: packages/MLX/Dockerfile.mlx
#     env_file:
#       - .env
#     volumes:
#       - ./packages/MLX/MLX:/app/mlx
#       - ./models:/models
#       - mlx-build:/app/mlx
#       - ./shared/:app/shared
#     command: python /app/mlx/server.py
#     networks
  # - aigency-network
#     hostname: mlx
# vllm:
#   image: twgsportsclub/vllm
#   build:
#     context: ./packages/vllm-server
#     dockerfile: Dockerfile.vllm
#   ports:
#     - "${VLLM_PORT:-8000}:8000"
#   environment:
#     - VLLM_MODEL=deepseek-ai/deepseek-coder-6.7b-instruct
#   deploy:
#     resources:
#       limits:
#         memory: 16G
#       reservations:
#         memory: 12G
#   networks:
#     - aigency-net
#   ollama:
#     image: ollama/ollama:latest
#     ports:
#       - "11434:11434"
#     volumes:
#       - ollama-data:/root/.ollama
#       - ./check_ollama.sh:/check_ollama.sh
#     deploy:
#       resources:
#         limits:
#           memory: 16G  # Set an upper limit
#         reservations:
#           memory: 12G  # Guarantee this much memory
#     networks:
#       - ai-assistant-network
#     healthcheck:
#       test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 40s
#     environment:
#       - OLLAMA_HOST=0.0.0.0
#     # command: sh -c "ollama serve"

#   searxng:
#     image: searxng/searxng:latest
#     volumes:
#       - ./searxng:/etc/searxng:rw
#     ports:
#       - "4000:8080"
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   perplexica-backend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: backend.dockerfile
#       args:
#         - SEARXNG_API_URL=http://searxng:8080
#     depends_on:
#       - searxng
#       - ollama
#     ports:
#       - "3001:3001"
#     volumes:
#       - backend-dbstore:/home/perplexica/data
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   perplexica-frontend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: app.dockerfile
#       args:
#         - NEXT_PUBLIC_API_URL=http://127.0.0.1:3001/api
#         - NEXT_PUBLIC_WS_URL=ws://127.0.0.1:3001
#     depends_on:
#       - perplexica-backend
#     ports:
#       - "3000:3000"
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   aider:
#     build:
#       context: ./packages/aider
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       ollama:
#           condition: service_healthy
#     stdin_open: true
#     tty: true
#     ports:
#       - "8501:8501"
#     volumes:
#       - .:/app  # Mount the entire monorepo
#       - ./packages/aider/start_aider.sh:/start_aider.sh  # Mount the start script
#     deploy:
#       resources:
#         limits:
#           memory: 16G  # Set an upper limit
#         reservations:
#           memory: 12G  # Guarantee this much memory
#     working_dir: /app/packages/aider  # Set the working directory to the aider package
#     environment:
#       - OLLAMA_API_BASE=http://ollama:11434
#       # - AIDER_MODEL=deepseek-coder-v2:latest
#       - AIDER_MODEL=codellama:7b
#       - SKIP_AIDER_UPDATE=false
#     command: /start_aider.sh
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   agent-zero:
#     build:
#       context: ./packages/agent-zero/docker
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     stdin_open: true
#     tty: true
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"
#     ports:
#       - "5001:5001"
#       - "2222:22"
#     volumes:
#       - ./agent-zero/work_dir:/home/agentuser/work_dir
#     environment:
#       - OLLAMA_BASE_URL=http://ollama:11434
#     cap_add:
#       - SYS_ADMIN
#     # security_opt:
#     #   - seccomp:unconfined
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   open-hands:
#     image:
#       ghcr.io/all-hands-ai/openhands:main
#     environment:
#       - SANDBOX_USER_ID=${SANDBOX_USER_ID:-$(id -u)}  # Use the current user ID
#       - WORKSPACE_DIR="./packages/open-hands/"
#     volumes:
#       # - ${WORKSPACE_BASE:-$(pwd)/workspace}:/opt/workspace_base

#       - /var/run/docker.sock:/var/run/docker.sock
#     ports:
#       - "3333:3333"
#       - "3000:3333"
#     command: uvicorn openhands.server.listen:app --host 0.0.0.0 --port 3333
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

# networks:
#   ai-assistant-network:

# volumes:
#   backend-dbstore:
#   ollama-data:

# services:
#   ollama:
#     image: docker.io/ollama/ollama:latest
#     ports:
#       - 11434:11434
#   searxng:
#     image: docker.io/searxng/searxng:latest
#     volumes:
#       - ./searxng:/etc/searxng:rw
#     ports:
#       - 4000:8080
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   perplexica-backend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: backend.dockerfile
#       args:
#         - SEARXNG_API_URL=http://searxng:8080
#     depends_on:
#       - searxng
#     ports:
#       - 3001:3001
#     volumes:
#       - backend-dbstore:/home/perplexica/data
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   perplexica-frontend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: app.dockerfile
#       args:
#         - NEXT_PUBLIC_API_URL=http://127.0.0.1:3001/api
#         - NEXT_PUBLIC_WS_URL=ws://127.0.0.1:3001
#     depends_on:
#       - perplexica-backend
#     ports:
#       - 3000:3000
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   aider:
#     build:
#       context: ./packages/aider
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     ports:
#       - 5555:5555
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   agent-zero:
#     build:
#       context: ./packages/agent-zero/docker
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     ports:
#       - 5001:5001
#     networks:
#       - perplexica-network
#     restart: unless-stopped

# networks:
#   perplexica-network:

# volumes:
#   backend-dbstore:

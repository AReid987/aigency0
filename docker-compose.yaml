services:
  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    hostname: aigency-tailnet
    volumes:
      - ./tailscale:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    environment:
      - TS_AUTHKEY=tskey-auth-kn4443UWS611CNTRL-TX74yvREWSAbjeZLKmFySAh7RggBFZ5XP
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_ROUTES=10.0.0.0/8
      - TS_EXTRA_ARGS=--hostname=aigency-tailnet
    network_mode: host
    restart: unless-stopped
  nginx:
    image: nginx
    hostname: nginx
    network_mode: service:tailscale
  dask-scheduler:
    image: twgsportsclub/dask-docker-arm64
    platform: linux/arm64
    command: ["dask-scheduler"]
    network_mode: service:tailscale
    hostname: dask-scheduler
  dask-worker:
    image: twgsportsclub/dask-docker-arm64
    platform: linux/arm64
    command: ["dask-worker", "tcp://dask-scheduler.tailnet:8786"]
    depends_on:
      - dask-scheduler
    deploy:
      replicas: ${DASK_WORKERS:-1}
    network_mode: service:tailscale
  llama-cpp:
    image: llama-cpp-server:latest
    build:
      context: ./packages/llama.cpp
      dockerfile: Dockerfile.llama-cpp
    ports:
      - "5001:5001"
    volumes:
      - ./packages/llama.cpp/models:/models
    environment:
      - FLASK_ENV=development
    restart: unless-stopped
  ollama:
    image: ollama/ollama
    volumes:
      - ./ollama-models:/root/.ollama
    network_mode: service:tailscale
  aider:
    image: paulgauthier/aider-full
    pull_policy: always
    volumes:
      - .:/app
    environment:
      # - GROQ_API_KEY=${GROQ_API_KEY}
      - OLLAMA_API_BASE=http://ollama.tailnet:11434
      - MLX_API_BASE=http://mlx.tailnet:5000
      - LLM_API_URL=http://mlx.tailnet:5000/generate
    depends_on:
      - ollama
      - mlx
    command: --cache-prompts --gui --model ollama/deepseek-coder-v2
    network_mode: service:tailscale
    hostname: aider
  mlx:
    build:
      context: ./packages/mlx
      dockerfile: Dockerfile.mlx
    environment:
      - USE_MLX=true
    volumes:
      - ./packages/MLX/mlx:/app/mlx
      - ./models:/models
      - mlx-build:/app/mlx
    command: python /app/mlx/server.py
    network_mode: service:tailscale
    hostname: mlx
  pytorch:
    build:
      context: ./packages/pytorch
      dockerfile: Dockerfile.pytorch
    volumes:
      - .:/app
    environment:
      - USE_MLX=false
    command: python server.py
volumes:
  mlx-build:
#   tailscale:
#     driver: local
# networks:
#   aigency-net:
#     driver: bridge
#       - aigency-net
# vllm:
#   image: twgsportsclub/vllm
#   build:
#     context: ./packages/vllm-server
#     dockerfile: Dockerfile.vllm
#   ports:
#     - "${VLLM_PORT:-8000}:8000"
#   environment:
#     - VLLM_MODEL=deepseek-ai/deepseek-coder-6.7b-instruct
#   deploy:
#     resources:
#       limits:
#         memory: 16G
#       reservations:
#         memory: 12G
#   networks:
#     - aigency-net
#   ollama:
#     image: ollama/ollama:latest
#     ports:
#       - "11434:11434"
#     volumes:
#       - ollama-data:/root/.ollama
#       - ./check_ollama.sh:/check_ollama.sh
#     deploy:
#       resources:
#         limits:
#           memory: 16G  # Set an upper limit
#         reservations:
#           memory: 12G  # Guarantee this much memory
#     networks:
#       - ai-assistant-network
#     healthcheck:
#       test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
#       interval: 30s
#       timeout: 10s
#       retries: 5
#       start_period: 40s
#     environment:
#       - OLLAMA_HOST=0.0.0.0
#     # command: sh -c "ollama serve"

#   searxng:
#     image: searxng/searxng:latest
#     volumes:
#       - ./searxng:/etc/searxng:rw
#     ports:
#       - "4000:8080"
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   perplexica-backend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: backend.dockerfile
#       args:
#         - SEARXNG_API_URL=http://searxng:8080
#     depends_on:
#       - searxng
#       - ollama
#     ports:
#       - "3001:3001"
#     volumes:
#       - backend-dbstore:/home/perplexica/data
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   perplexica-frontend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: app.dockerfile
#       args:
#         - NEXT_PUBLIC_API_URL=http://127.0.0.1:3001/api
#         - NEXT_PUBLIC_WS_URL=ws://127.0.0.1:3001
#     depends_on:
#       - perplexica-backend
#     ports:
#       - "3000:3000"
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   aider:
#     build:
#       context: ./packages/aider
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       ollama:
#           condition: service_healthy
#     stdin_open: true
#     tty: true
#     ports:
#       - "8501:8501"
#     volumes:
#       - .:/app  # Mount the entire monorepo
#       - ./packages/aider/start_aider.sh:/start_aider.sh  # Mount the start script
#     deploy:
#       resources:
#         limits:
#           memory: 16G  # Set an upper limit
#         reservations:
#           memory: 12G  # Guarantee this much memory
#     working_dir: /app/packages/aider  # Set the working directory to the aider package
#     environment:
#       - OLLAMA_API_BASE=http://ollama:11434
#       # - AIDER_MODEL=deepseek-coder-v2:latest
#       - AIDER_MODEL=codellama:7b
#       - SKIP_AIDER_UPDATE=false
#     command: /start_aider.sh
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   agent-zero:
#     build:
#       context: ./packages/agent-zero/docker
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     stdin_open: true
#     tty: true
#     logging:
#       driver: "json-file"
#       options:
#         max-size: "10m"
#         max-file: "3"
#     ports:
#       - "5001:5001"
#       - "2222:22"
#     volumes:
#       - ./agent-zero/work_dir:/home/agentuser/work_dir
#     environment:
#       - OLLAMA_BASE_URL=http://ollama:11434
#     cap_add:
#       - SYS_ADMIN
#     # security_opt:
#     #   - seccomp:unconfined
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

#   open-hands:
#     image:
#       ghcr.io/all-hands-ai/openhands:main
#     environment:
#       - SANDBOX_USER_ID=${SANDBOX_USER_ID:-$(id -u)}  # Use the current user ID
#       - WORKSPACE_DIR="./packages/open-hands/"
#     volumes:
#       # - ${WORKSPACE_BASE:-$(pwd)/workspace}:/opt/workspace_base

#       - /var/run/docker.sock:/var/run/docker.sock
#     ports:
#       - "3333:3333"
#       - "3000:3333"
#     command: uvicorn openhands.server.listen:app --host 0.0.0.0 --port 3333
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - ai-assistant-network
#     restart: unless-stopped

# networks:
#   ai-assistant-network:

# volumes:
#   backend-dbstore:
#   ollama-data:

# services:
#   ollama:
#     image: docker.io/ollama/ollama:latest
#     ports:
#       - 11434:11434
#   searxng:
#     image: docker.io/searxng/searxng:latest
#     volumes:
#       - ./searxng:/etc/searxng:rw
#     ports:
#       - 4000:8080
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   perplexica-backend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: backend.dockerfile
#       args:
#         - SEARXNG_API_URL=http://searxng:8080
#     depends_on:
#       - searxng
#     ports:
#       - 3001:3001
#     volumes:
#       - backend-dbstore:/home/perplexica/data
#     extra_hosts:
#       - 'host.docker.internal:host-gateway'
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   perplexica-frontend:
#     build:
#       context: ./packages/Perplexica
#       dockerfile: app.dockerfile
#       args:
#         - NEXT_PUBLIC_API_URL=http://127.0.0.1:3001/api
#         - NEXT_PUBLIC_WS_URL=ws://127.0.0.1:3001
#     depends_on:
#       - perplexica-backend
#     ports:
#       - 3000:3000
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   aider:
#     build:
#       context: ./packages/aider
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     ports:
#       - 5555:5555
#     networks:
#       - perplexica-network
#     restart: unless-stopped

#   agent-zero:
#     build:
#       context: ./packages/agent-zero/docker
#       dockerfile: Dockerfile
#       args:
#         - PYTHON_VERSION=3.12
#     depends_on:
#       - ollama
#     ports:
#       - 5001:5001
#     networks:
#       - perplexica-network
#     restart: unless-stopped

# networks:
#   perplexica-network:

# volumes:
#   backend-dbstore:

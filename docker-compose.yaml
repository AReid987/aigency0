services:
  # caddy:
  #   image: caddy
  #   # ports:
  #   #   - "80:80"
  #   #   - "443:443"
  #   volumes:
  #     - ./Caddyfile:/etc/caddy/Caddyfile
  #     - caddy_data:/data
  #     - caddy_config:/config
  #   network_mode: service:tailscale
  #   container_name: caddy
    # depends_on:
    #   - unified-dashboard
  # unified-dashboard:
  #   build:
  #     context: ./packages/dashboard
  #     dockerfile: Dockerfile
  #   volumes:
  #     - ./dashboard:/app
  #   environment:
  #     - NODE_ENV=production
  #   network_mode: service:tailscale
  #   hostname: unified-dashboard
  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    restart: unless-stopped
    hostname: aigency-tailnet
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    env_file:
      - .env
    environment:
      - TS_HOSTNAME=aigency-tailnet
      - TS_AUTHKEY=${TS_KEY}
      - TS_SERVE_CONFIG=/packages/config/nginx.json
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_ROUTES=10.0.0.0/12
      - TS_USERSPACE=false
      - "TS_EXTRA_ARGS=--hostname=aigency-tailnet --advertise-tags=tag:aigency --exit-node=aigency-tailnet --advertise-exit-node --exit-node-allow-lan-access --advertise-routes=10.0.0.0/12 --ssh --ts-auth=${TS_KEY}"
    network_mode: host
    volumes:
      - ./tailscale:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
  nginx:
    image: nginx
    container_name: tailscale-web
    network_mode: service:tailscale
  litellm:
    image: ghcr.io/berriai/litellm-database:main-stable
    restart: unless-stopped
    env_file:
      - .env
    environment:
      # Change POSTGRES_PORT to 6543 for Transaction Mode for Supabase connection pooler
      DATABASE_URL: ${DATABASE_URL}
      MASTER_KEY: ${MASTER_KEY}
      UI_USERNAME: ${UI_USERNAME}
      UI_PASSWORD: ${UI_PASSWORD}
      TRACELOOP_API_KEY: ${TRACELOOP_API_KEY}
      LITELLM_SALT_KEY: ${LITELLM_SALT_KEY}
      STORE_MODEL_IN_DB: "True"
    network_mode: service:tailscale
    volumes:
      - ./packages/litellm/config.yaml:/app/config.yaml
    entrypoint: []
    command: >
      /bin/sh -c "
      pip install traceloop-sdk &&
      litellm --config /app/config.yaml --port 4444 --num_workers 8
      "
  dask-scheduler:
    image: twgsportsclub/dask-docker-arm64
    container_name: dask-scheduler
    platform: linux/arm64
    network_mode: service:tailscale
    command: ["dask", "scheduler"]
  dask-worker:
    image: twgsportsclub/dask-docker-arm64
    container_name: dask-worker
    platform: linux/arm64
    deploy:
      replicas: ${DASK_WORKERS:-1}
    depends_on:
      - dask-scheduler
    network_mode: service:tailscale
    command: ["dask", "worker", "localhost:8786"]
  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - ./ollama_models:/root/.ollama
    network_mode: service:tailscale
  pull-model:
    image: genai-stack/pull-model:latest
    container_name: pull-model
    build:
      context: ./packages/ollama
      dockerfile: Dockerfile.pull_model
    tty: true
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://aigency-tailnet:11434}
      - LLM=${LLM:-codellama:7b}
    network_mode: service:tailscale
  aider:
    image: paulgauthier/aider-full
    pull_policy: always
    container_name: aider
    environment:
      # - GROQ_API_KEY=${GROQ_API_KEY}
      - OLLAMA_API_BASE=http://aigency-tailnet:11434
      - MLX_API_BASE=http://host.docker.internal:5555
      - LLM_API_URL=http://host.docker.internal:5555/predict
      - ollama
    network_mode: service:tailscale
    depends_on:
      - ollama
    volumes:
      - .:/app
    command: --cache-prompts --gui --model ollama/deepseek-coder-v2
  pytorch:
    build:
      context: ./packages/pytorch
      dockerfile: Dockerfile.pytorch
    container_name: pytorch
    env_file:
      - .env
    volumes:
      - .:/app
    command: python server.py
  redis:
    image: redis:alpine
    container_name: redis
    volumes:
      - redis_data:/data
    network_mode: service:tailscale
volumes:
  tailscale:
    driver: local
  redis_data:
  ollama_models:
  caddy_data:
  caddy_config:
